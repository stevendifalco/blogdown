[{"authors":["admin"],"categories":null,"content":"I am currently pursuing a master’s degree in Natural Resources at the University of Connecticut, where I am studying the human dimensions of roadside vegetation management. My research project involved a survey mailed to residents of Connecticut, to understand public perceptions and opinons towards tree management. I used spatial analysis to evaluate how attitudes vary spatially. I am interested in the interaction between humans and natural resources, and how this shapes their approval of management decisions. While at UConn, I have honed my skills in ArcGIS/ArcOnline and developed an understanding of statistical softwares such as R, Python, and SPSS.\nFor five years, I worked as a natural resource specialist to restore native ecosystems through invasive plant management. I focused on restoring forests and fields to their native composition. I collected and propogated native plants to aid in this process, and return locally extripated species. I am interested in the way exotic invasive species affect local ecosystems, and best management practices to mitigate their impacts. One of the most important parts of land management is the reestablishment of native plant populations. Restoring and maintaining native plant populations is important for helping provide habitat for our native pollinators\n Interview with Naturally@UConn\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://stevendifalco.com/author/steven-difalco/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/steven-difalco/","section":"authors","summary":"I am currently pursuing a master’s degree in Natural Resources at the University of Connecticut, where I am studying the human dimensions of roadside vegetation management. My research project involved a survey mailed to residents of Connecticut, to understand public perceptions and opinons towards tree management.","tags":null,"title":"Steven DiFalco","type":"authors"},{"authors":["steven"],"categories":["tidytuesday","data visualization"],"content":" #Modes of travling biking and walking ACS data commute_mode \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-05/commute.csv\u0026quot;) ## Parsed with column specification: ## cols( ## city = col_character(), ## state = col_character(), ## city_size = col_character(), ## mode = col_character(), ## n = col_double(), ## percent = col_double(), ## moe = col_double(), ## state_abb = col_character(), ## state_region = col_character() ## ) commute_mode$state \u0026lt;- recode(commute_mode$state, \u0026quot;Ca\u0026quot;= \u0026quot;California\u0026quot;, \u0026quot;Massachusett\u0026quot; = \u0026quot;Massachusetts\u0026quot;) #summarise data each state for percent biking and walking commute_summary \u0026lt;- commute_mode %\u0026gt;% mutate(state = tolower(state)) %\u0026gt;% group_by(state, mode) %\u0026gt;% summarise(Percent = mean(percent)) ## `summarise()` regrouping output by \u0026#39;state\u0026#39; (override with `.groups` argument) #retrieve state geo data states_map \u0026lt;- map_data(\u0026quot;state\u0026quot;) #filter by Northeastern States NE_states \u0026lt;- subset(states_map, region %in% c(\u0026quot;connecticut\u0026quot;, \u0026quot;massachusetts\u0026quot;,\u0026quot;maine\u0026quot;, \u0026quot;new hampshire\u0026quot;, \u0026quot;new york\u0026quot;, \u0026quot;rhode island\u0026quot;, \u0026quot;vermont\u0026quot;)) #plot by new england states commute_summary %\u0026gt;% ggplot(aes(map_id = state)) + geom_map(aes(fill=Percent), map = NE_states)+ #sets up map facet_wrap(vars(mode)) + #displays both bike and walk on same figure expand_limits(x= NE_states$long, y=NE_states$lat)+ #sets limits based on lat/long of states file coord_map(\u0026quot;polyconic\u0026quot;) + scale_fill_viridis(option = \u0026quot;D\u0026quot;) + theme_void()+ #gets rid of xy grid labs(fill = \u0026quot;Percent of commuters\u0026quot;, title= \u0026quot;The Northeast Loves Walking\u0026quot;)+ theme(legend.position=\u0026quot;bottom\u0026quot;, plot.title = element_text(hjust =0.5), strip.text.x = element_text(size = 12))#changes text of mode commute_summary ## # A tibble: 102 x 3 ## # Groups: state [51] ## state mode Percent ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 alabama Bike 0.235 ## 2 alabama Walk 1.33 ## 3 alaska Bike 1.43 ## 4 alaska Walk 5.03 ## 5 arizona Bike 0.724 ## 6 arizona Walk 2.10 ## 7 arkansas Bike 0.155 ## 8 arkansas Walk 2.05 ## 9 california Bike 0.983 ## 10 california Walk 2.36 ## # … with 92 more rows ","date":1591660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591660800,"objectID":"c521fa32a53af828267f0b5e5b25c752","permalink":"https://stevendifalco.com/post/2020-06-09/","publishdate":"2020-06-09T00:00:00Z","relpermalink":"/post/2020-06-09/","section":"post","summary":"tidy tuesday walkability","tags":null,"title":"Walkability","type":"post"},{"authors":null,"categories":["TidyTuesday"],"content":"This is my contribution to TidyTuesday\nIn this, I’m attempting to use the gganimate package for the first time to create an animation showing volcanoe eruptions over the past century and their location. Packages used for this document: library(readxl) library(tidyverse) library(ggplot2) library(gganimate) library(maps) library(ggthemes) library(DT) library(gifski)  Lets take a look at the data first and see what information is here.\neruptions \u0026lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/eruptions.csv') eruption_list \u0026lt;- eruptions %\u0026gt;% janitor::clean_names() %\u0026gt;% select(-contains(\u0026quot;modifier\u0026quot;), -contains(\u0026quot;uncertainty\u0026quot;)) %\u0026gt;% filter(eruption_category == \u0026quot;Confirmed Eruption\u0026quot;) %\u0026gt;% #only confirmed volcanoes stay in data drop_na(end_year) %\u0026gt;% #drops those without end year filter(end_year \u0026gt; 2010) #filters only volcanoes after 2010 datatable(eruption_list, rownames = FALSE, options = list(pageLength = 5))  Next, I’ll create an underlying map for plotting the points.\nworld \u0026lt;- ggplot() + borders(\u0026quot;world\u0026quot;, colour = \u0026quot;gray85\u0026quot;, fill = \u0026quot;gray80\u0026quot;) + theme_map() map \u0026lt;- world + geom_point(aes(x = longitude, y = latitude, size = vei), data = eruption_list, colour = 'purple', alpha = .5) + scale_size_continuous(range = c(1, 7)) + labs(size = 'Volcanic Explosivity Index') map  Now let’s animate! I did not realize going into this that the points would ‘move’ from year to year instead of appearing for that year then disappear. Something to consider for the next time I use this package.\nanimate2 \u0026lt;- world + geom_point(aes(x = longitude, y = latitude, size = vei), data = eruption_list, colour = 'purple', alpha = .5) + labs(title = \u0026quot;Date: {frame_time}\u0026quot;, size = \u0026quot;Volcanic Index\u0026quot;) + transition_time(end_year) + ease_aes(\u0026quot;linear\u0026quot;) animate(animate, renderer = gifski_renderer(\u0026quot;volcanobyyear.gif\u0026quot;))  ","date":1591228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591228800,"objectID":"58ac9f84c12ea31593b045f613a1f278","permalink":"https://stevendifalco.com/post/2020-06-04/volcanos-tidytuesday-data/","publishdate":"2020-06-04T00:00:00Z","relpermalink":"/post/2020-06-04/volcanos-tidytuesday-data/","section":"post","summary":"This is my contribution to TidyTuesday\nIn this, I’m attempting to use the gganimate package for the first time to create an animation showing volcanoe eruptions over the past century and their location.","tags":["R","R Studio","data visualizations","animation","TidyTuesday"],"title":"Volcanos Data TidyTuesday","type":"post"},{"authors":null,"categories":null,"content":"1) Bokeh- Python package Using the Bokeh package in Python to visualize local trails and protected open space parcels. These are dynamic maps which can be zoomed into and scrolled around. Hovering over any given trail will provide the property name. Open space parcels will reveil parcel ID when hovered over.  Mansfield Town Trails and Protected Open Space 2) iNaturalist data analysis in R I am interested in using iNaturalist data to understand distribution of species and what species are being represented by citizen scientists. There is an overwhelming amount of data collected everyday. Learning ways to utilize this data is important. I\u0026rsquo;ve been working on different ways to visualize iNat observations. Below are some preliminary results. I would like to understand if invasive species or native species are posted more often and where these observations occur; are there more observations near homes or protected lands? how far off trail do people go to make observations? Top 30 most common plants Top 30 most common observations for 10 years of iNaturalist data, separated by native and invasive. It appears that many of top observed plants are abundant species throughout most of Connecticut and mostly deer-resistant species. Native vs Invasive Observations occured throughout most of the state during this 10 year period, with no apparent trend yet of invasive vs native plants. Of interest to me is the abundance of observations around specific areas, such as the UConn campus in the northeastern part of the state. ","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"b21dae5e5d6772ecf7b46d8ed4330bbf","permalink":"https://stevendifalco.com/projects/inaturalist/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/projects/inaturalist/","section":"projects","summary":"Examples of other projects using Python for a mapping project and iNaturalist data.","tags":["inatrualist","data visualizations","python","jypter notebook"],"title":"Other Projects","type":"projects"},{"authors":null,"categories":null,"content":"Here are my contributions to #TidyTuesday 2020 Visualization of the wildfires in Australia\u0026rsquo;s New South Wales province. Week 1 - Australlian Wilfires\n2019 Density estimation of squirrels in Central Park. Week 44 - Squirrel Census\nPercentage of commuters who walk to work by state. Week 45 - Modes of Commuting\n","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591056000,"objectID":"724c3b37c52ee8cd831c838c3931e812","permalink":"https://stevendifalco.com/projects/tidytuesday/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/projects/tidytuesday/","section":"projects","summary":"Examples of using open-source data to make visualizations in R.","tags":["data visualizations","R studio","R","mapping"],"title":"Tidy Tuesday","type":"projects"},{"authors":null,"categories":["breakfast","granola"],"content":"Crunchy Granola May 28, 2020\nThis is an adapted version of this recipe by Cooking Classy. I generally make a massive amount of granola every two weeks and eat it for breakfast daily with soy milk. Ingredients  8 cups oats, not quick cooking oats 1 cup pumpkin seeds 1 cup sunflower seeds 1/2- 3/4 cup brown sugar (sometimes with honey, agave, or maple syrup) 2 Tbsp water 2 tsp ground cinnamon 1/4 tsp nutmeg 1/4 tsp salt 1 tsp vanilla extract 1/4 tsp baking soda  add-ins when out of oven  1 cup raisins or craisns 1/2 cup toasted coconut flakes (baking with this mixed in did not come out super well)  Instructions   Preheat oven to 300 degrees. Mix oats and nuts together. Leave out dried fruit until after it\u0026rsquo;s out of the oven.\n  In a medium saucepan, combine brown sugar, water, cinnamon, and salt. Stir frequently until it comes to a boil, and boil for 1 minute.\n  Take off heat, stir in vanilla and baking soda. Whisk until mixture gets foamy. Pour over oats immediately and toss to cover (really just mix real well).\n  Spread oats onto baking sheet (usually I need two for this recipe) with parchment paper underneath.\n  Bake in preheated oven, about 15-30 minutes until lightly golden brown, removing from oven and tossing during baking.\n  Once finished, take out of oven and allow to cool before storing in a container.\n  ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"720e567e3e84900495110789f6598636","permalink":"https://stevendifalco.com/recipes/crunchy-granola/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/recipes/crunchy-granola/","section":"Recipes","summary":"Crunchy Granola May 28, 2020\nThis is an adapted version of this recipe by Cooking Classy. I generally make a massive amount of granola every two weeks and eat it for breakfast daily with soy milk.","tags":["recipes","breakfast"],"title":"Crunchy Granola","type":"Recipes"},{"authors":null,"categories":null,"content":"This interactive map application allows the user to select different attributes (such as trails) to find out more information and directions to the location. This project was done in collaboration with Joshua\u0026rsquo;s Trust and the town of Mansfield, CT. Pulling together town, state, and land trust trails to one place and provide useful information for those who would like to recreate in the Mansfield, CT. Created using ArcGIS Online.  .embed-container {position: relative; padding-bottom: 80%; height: 0; max-width: 100%;} .embed-container iframe, .embed-container object, .embed-container iframe{position: absolute; top: 0; left: 0; width: 100%; height: 100%;} small{position: absolute; z-index: 40; bottom: 0; margin-bottom: -15px;} ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"bee781fb4cbe8582069b4524b62b3bcd","permalink":"https://stevendifalco.com/projects/mansfieldmapper/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/projects/mansfieldmapper/","section":"projects","summary":"Trail map for the town of Mansfield, CT made with ArcOnline.","tags":["mapping","Mansfield","Connecticut","ArcGIS","ArcOnline"],"title":"Mansfield Trail Mapper","type":"projects"},{"authors":null,"categories":["dinner","quinoa","black bean"],"content":"Quinoa Black Bean Burgers May 28, 2020\nThis is an adapted version of this recipe. I doubled the recipe and made some adjustments to use my own cooked beans. Ingredients  2 cups cooked black beans (rinsed and drained) 1 cup cooked quinoa 1 cup bread crumbs (I used plain gluten free) 4 tablespoons minced onion 1 large clove garlic, minced 1 teaspoon garlic powder 1 teaspoon onion powder 1 1/2 teaspoons ground cumin 1/2 teaspoon salt 2 teaspoon hot sauce 1 egg 3 tablespoons canola oil  Directions  (skip if quinoa is already cooked) Bring the quinoa and water to a boil in a saucepan. Reduce heat to medium-low, cover, and simmer until the quinoa is tender and the water has been absorbed, about 15 to 20 minutes. Roughly mash the black beans with a fork leaving some whole black beans in a paste-like mixture. Mix the quinoa, bread crumbs, onion, garlic, cumin, salt, hot sauce, and egg into the black beans using your hands. Form the black bean mixture into patties. If the patties do not form, can try adding more black beans, bread crumbs, or another egg depending on the consistency. Heat the oil in a large skillet. Cook the patties in the hot oil until heated through, 2 to 3 minutes per side Add siracha mayo sauce and enjoy!  ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"da77893a8a5ea55489fdad87d17e3cfe","permalink":"https://stevendifalco.com/recipes/quinoa-black-bean-burgers/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/recipes/quinoa-black-bean-burgers/","section":"Recipes","summary":"Quinoa Black Bean Burgers May 28, 2020\nThis is an adapted version of this recipe. I doubled the recipe and made some adjustments to use my own cooked beans. Ingredients  2 cups cooked black beans (rinsed and drained) 1 cup cooked quinoa 1 cup bread crumbs (I used plain gluten free) 4 tablespoons minced onion 1 large clove garlic, minced 1 teaspoon garlic powder 1 teaspoon onion powder 1 1/2 teaspoons ground cumin 1/2 teaspoon salt 2 teaspoon hot sauce 1 egg 3 tablespoons canola oil  Directions  (skip if quinoa is already cooked) Bring the quinoa and water to a boil in a saucepan.","tags":["recipes"],"title":"Quinoa Black Bean Burgers","type":"Recipes"},{"authors":["alison"],"categories":["readr","readxl","data import"],"content":" A shorter version of this blog post now appears as an article vignette for the readxl package, thank you to Jenny Bryan for the invitation!\nA problem I run up against a lot when working with other people’s data is having multiple header rows in the source data file. I like to use readr functions to read in rectangular data like .csv and .tsv files, but if you skip rows at import using the skip argument, you lose the header row as well, which usually has column names. The problem I often have is that the header row has column names that I want to keep, but I’d like to skip the second row (or more), which has some junk in it. Usually this row is some kind of data dictionary inserted between the row of column names and the actual data.\nIn this post, I’ll walk through a solution to this problem, using the readr package. You can also watch along in the video.\nWarning!: I made a mistake when I said readr uses the first 100 rows of your data to predict column types- it uses the first 1000 rows.\nBeing sticker rich This dataset is from an article published in PLOS ONE called “Being Sticker Rich: Numerical Context Influences Children’s Sharing Behavior”. In this study, children (ages 3–11) received a small (12, “sticker poor”) or large (30, “sticker rich”) number of stickers, and were then given the opportunity to share their windfall with either one or multiple anonymous recipients. This type of experimental design is a version of the Dictator Game.\nThe main research questions the authors explored were: do the number of available resources and/or the number of potential recipients alter the likelihood of a child donating and/or the amount they donate? But, in order to answer this question, we have to be able to read in the data! Luckily, these lovely developmental psychologists opted to share their data on the Harvard Dataverse as a tab-delimited file.\nIf you download the file, you can open it up in a plain text editor. You can also open it with Microsoft Excel.  Read in the file Let’s start by creating a variable called link to store the link to the data file.\n# create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; The file has a .tab extension, so we know it is tab-delimited. This means that the right readr function for reading this file is read_tsv. Since we stored our link already as a character string, that is the only argument to the read_tsv function.\n#install.packages(\u0026quot;readr\u0026quot;) library(readr) # load the readr package stickers \u0026lt;- read_tsv(link) # spec() Now, we know the second row of data is wonky, but how can we see that in R? There are a number of ways we can go spelunking around into our data file. The easiest to print it. Since we used readr, we have a tibble, which nicely prints to screen.\nstickers # # A tibble: 402 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 [Included Sa… 1=12:1; … 1=12; 2=30 1=1 recipient;… 1=fem… NA # 2 1 1 1 1 1 36 # 3 2 1 1 1 2 36 # 4 3 1 1 1 2 36 # 5 4 1 1 1 1 36 # 6 5 1 1 1 2 36 # 7 6 1 1 1 2 36 # 8 7 2 1 2 1 36 # 9 8 2 1 2 2 36 # 10 9 3 2 1 2 36 # # … with 392 more rows, and 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; Unfortunately, dplyr::glimpse can’t help us much, because we have one variable name that is ridiculously long (absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)). We’ll fix that with dplyr::rename.\nlibrary(dplyr) glimpse(stickers) # Rows: 402 # Columns: 18 # $ SubjectNumber \u0026lt;chr\u0026gt; … # $ Condition \u0026lt;chr\u0026gt; … # $ NumberStickers \u0026lt;chr\u0026gt; … # $ NumberEnvelopes \u0026lt;chr\u0026gt; … # $ Gender \u0026lt;chr\u0026gt; … # $ Agemonths \u0026lt;dbl\u0026gt; … # $ Ageyears \u0026lt;dbl\u0026gt; … # $ Agegroups \u0026lt;chr\u0026gt; … # $ `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt; … # $ LeftEnvelope \u0026lt;chr\u0026gt; … # $ RightEnvelope \u0026lt;chr\u0026gt; … # $ `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt; … # $ `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt; … # $ Giveornot \u0026lt;chr\u0026gt; … # $ LargerEnvelopeabs \u0026lt;chr\u0026gt; … # $ LargeEnvelopepercent \u0026lt;chr\u0026gt; … # $ SmallerEnvelopeabs \u0026lt;chr\u0026gt; … # $ SmallEnvelopepercent \u0026lt;chr\u0026gt; … More options:\nhead(stickers) # # A tibble: 6 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 [Included Sa… 1=12:1; … 1=12; 2=30 1=1 recipient;… 1=fem… NA # 2 1 1 1 1 1 36 # 3 2 1 1 1 2 36 # 4 3 1 1 1 2 36 # 5 4 1 1 1 1 36 # 6 5 1 1 1 2 36 # # … with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; tail(stickers) # # A tibble: 6 x 18 # SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; # 1 396 1 1 1 2 136 # 2 397 4 2 2 1 136 # 3 398 1 1 1 1 137 # 4 399 1 1 1 2 137 # 5 400 4 2 2 2 139 # 6 401 3 2 1 1 143 # # … with 12 more variables: Ageyears \u0026lt;dbl\u0026gt;, Agegroups \u0026lt;chr\u0026gt;, # # `Subject\u0026#39;sEnvelope` \u0026lt;chr\u0026gt;, LeftEnvelope \u0026lt;chr\u0026gt;, RightEnvelope \u0026lt;chr\u0026gt;, # # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` \u0026lt;chr\u0026gt;, # # `PercentGiven(Outof100percent)` \u0026lt;chr\u0026gt;, Giveornot \u0026lt;chr\u0026gt;, # # LargerEnvelopeabs \u0026lt;chr\u0026gt;, LargeEnvelopepercent \u0026lt;chr\u0026gt;, # # SmallerEnvelopeabs \u0026lt;chr\u0026gt;, SmallEnvelopepercent \u0026lt;chr\u0026gt; names(stickers) # [1] \u0026quot;SubjectNumber\u0026quot; # [2] \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; # [4] \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; # [6] \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; # [8] \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; # [10] \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; # [12] \u0026quot;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; # [14] \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; # [16] \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; # [18] \u0026quot;SmallEnvelopepercent\u0026quot; # View() Now we are ready to diagnose the problem!\nProblem: the first row is not really data. It is metadata about the variables, and it is screwing up readr’s ability to predict our column types.\nSolution: we’ll use readr and the read_tsv() function to read in the data twice. In Step 1, we’ll create a character vector of the column names only. In Step 2, we’ll read in the actual data and skip the multiple header rows at the top. When we do this, we lose the column names, so we use the character vector of column names we created in Step 1 instead.\n Read in the file (again) Step 1 Goal: we want to read in the first row only and save it as a character vector called sticker_names. This row contains the correct column names that we’ll need in Step 2.\nsticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() sticker_names # [1] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; # [3] \u0026quot;NumberStickers\u0026quot; \u0026quot;NumberEnvelopes\u0026quot; # [5] \u0026quot;Gender\u0026quot; \u0026quot;Agemonths\u0026quot; # [7] \u0026quot;Ageyears\u0026quot; \u0026quot;Agegroups\u0026quot; # [9] \u0026quot;Subject\u0026#39;sEnvelope\u0026quot; \u0026quot;LeftEnvelope\u0026quot; # [11] \u0026quot;RightEnvelope\u0026quot; \u0026quot;stickersgiven\u0026quot; # [13] \u0026quot;PercentGiven(Outof100percent)\u0026quot; \u0026quot;Giveornot\u0026quot; # [15] \u0026quot;LargerEnvelopeabs\u0026quot; \u0026quot;LargeEnvelopepercent\u0026quot; # [17] \u0026quot;SmallerEnvelopeabs\u0026quot; \u0026quot;SmallEnvelopepercent\u0026quot; glimpse(sticker_names) # chr [1:18] \u0026quot;SubjectNumber\u0026quot; \u0026quot;Condition\u0026quot; \u0026quot;NumberStickers\u0026quot; \u0026quot;NumberEnvelopes\u0026quot; ...  Step 2 Goal: we want to read in all the rows except for the first two rows, which contained the variable names and variable descriptions. We want to save this as stickers, and set the column names to the sticker_names object we created in Step 1.\nstickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) glimpse(stickers) # Rows: 401 # Columns: 18 # $ SubjectNumber \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12… # $ Condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3… # $ NumberStickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2… # $ NumberEnvelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1… # $ Gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1… # $ Agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, 36, 3… # $ Ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… # $ Agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… # $ `Subject\u0026#39;sEnvelope` \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 30, 12… # $ LeftEnvelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18, 18,… # $ RightEnvelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, NA,… # $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18, 18,… # $ `PercentGiven(Outof100percent)` \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.33, 0… # $ Giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0… # $ LargerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA, NA,… # $ LargeEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, 1.… # $ SmallerEnvelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, NA,… # $ SmallEnvelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, 0.…   Fin! All together now: the final solution!\n# load packages library(readr) library(dplyr) # create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; # read in column names only sticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() # read in data, set column names stickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names)  Addendum For good measure, I would add a final step to everything above and use janitor::clean_names() to put all the variable names into snake case. So my final final solution is here:\n# load packages library(readr) library(dplyr) library(janitor) # create variable to store url link \u0026lt;- \u0026quot;https://dataverse.harvard.edu/api/access/datafile/2712105\u0026quot; # read in column names only sticker_names \u0026lt;- link %\u0026gt;% read_tsv(n_max = 0) %\u0026gt;% # default: col_names = TRUE rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% names() # read in data, set column names stickers \u0026lt;- link %\u0026gt;% read_tsv(skip = 2, col_names = sticker_names) %\u0026gt;% clean_names() stickers # # A tibble: 401 x 18 # subject_number condition number_stickers number_envelopes gender agemonths # \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; # 1 1 1 1 1 1 36 # 2 2 1 1 1 2 36 # 3 3 1 1 1 2 36 # 4 4 1 1 1 1 36 # 5 5 1 1 1 2 36 # 6 6 1 1 1 2 36 # 7 7 2 1 2 1 36 # 8 8 2 1 2 2 36 # 9 9 3 2 1 2 36 # 10 10 3 2 1 2 36 # # … with 391 more rows, and 12 more variables: ageyears \u0026lt;dbl\u0026gt;, agegroups \u0026lt;dbl\u0026gt;, # # subjects_envelope \u0026lt;dbl\u0026gt;, left_envelope \u0026lt;dbl\u0026gt;, right_envelope \u0026lt;dbl\u0026gt;, # # stickersgiven \u0026lt;dbl\u0026gt;, percent_given_outof100percent \u0026lt;dbl\u0026gt;, giveornot \u0026lt;dbl\u0026gt;, # # larger_envelopeabs \u0026lt;dbl\u0026gt;, large_envelopepercent \u0026lt;dbl\u0026gt;, # # smaller_envelopeabs \u0026lt;dbl\u0026gt;, small_envelopepercent \u0026lt;dbl\u0026gt; glimpse(stickers) # Rows: 401 # Columns: 18 # $ subject_number \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, … # $ condition \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3, … # $ number_stickers \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, … # $ number_envelopes \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, … # $ gender \u0026lt;dbl\u0026gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, … # $ agemonths \u0026lt;dbl\u0026gt; 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,… # $ ageyears \u0026lt;dbl\u0026gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, … # $ agegroups \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … # $ subjects_envelope \u0026lt;dbl\u0026gt; 7, 12, 4, 7, 12, 8, 8, 11, 26, 30, 12, … # $ left_envelope \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 2, 1, 4, 0, 18, 18, 0… # $ right_envelope \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, NA, N… # $ stickersgiven \u0026lt;dbl\u0026gt; 5, 0, 8, 5, 0, 4, 4, 1, 4, 0, 18, 18, 0… # $ percent_given_outof100percent \u0026lt;dbl\u0026gt; 0.42, 0.00, 0.67, 0.42, 0.00, 0.33, 0.3… # $ giveornot \u0026lt;dbl\u0026gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, … # $ larger_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 1, NA, NA, N… # $ large_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, 1.00… # $ smaller_envelopeabs \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 2, 0, NA, NA, N… # $ small_envelopepercent \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, 0.5000000, 0.00…  Bonus data dictionary As an extra bonus, when you do have extra header rows, you can create a data dictionary using the gather() function from the tidyr package.\nlibrary(tidyr) stickers_dict \u0026lt;- read_tsv(link, n_max = 1) %\u0026gt;% rename(stickersgiven = \u0026#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)\u0026#39;) %\u0026gt;% clean_names() %\u0026gt;% gather(variable_name, variable_description) stickers_dict # # A tibble: 18 x 2 # variable_name variable_description # \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; # 1 subject_number [Included Sample Only] # 2 condition 1=12:1; 2=12:2, 3=30:1, 4=30:2 # 3 number_stickers 1=12; 2=30 # 4 number_envelopes 1=1 recipient; 2=2 recipients # 5 gender 1=female; 2=male # 6 agemonths \u0026lt;NA\u0026gt; # 7 ageyears \u0026lt;NA\u0026gt; # 8 agegroups 1=3-4yrs; 2=5-6yrs; 3=7-8yrs; 4=9-11yrs # 9 subjects_envelope How many stickers did the child keep for themselves … # 10 left_envelope 1 recipient conditions: How many stickers the subjec… # 11 right_envelope 1 recipient conditions: N/A; 2 recipient conditions:… # 12 stickersgiven Regardless of condition, the number of stickers the … # 13 percent_given_outof100… Regardless of condition, the proportion of stickers … # 14 giveornot 1=Donated 1 or more stickers to the recipient(s); 0=… # 15 larger_envelopeabs Raw number of stickers (out of 30: Condition 2 or 4 … # 16 large_envelopepercent Proportion of stickers (out of 100%; Condition 2 or … # 17 smaller_envelopeabs Raw number of stickers (out of 30: Condition 2 or 4 … # 18 small_envelopepercent Proportion of stickers (out of 100%; Condition 2 or …  Useful resources  Great blog post from Lisa DeBruine using readxl to read in data with multiple header rows (including those with merged cells!): https://debruine.github.io/multirow_headers.html This GitHub issue with Hadley’s response that solved all my problems: https://github.com/tidyverse/readr/issues/179 My original tweet when I discovered this trick!   ","date":1531008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531008000,"objectID":"fa371094e1f6404895e5d7b630a34f51","permalink":"https://stevendifalco.com/post/untitled/","publishdate":"2018-07-08T00:00:00Z","relpermalink":"/post/untitled/","section":"post","summary":"Using the readr package to sidestep a common problem","tags":null,"title":"Read Data with Multiple Header Rows into R","type":"post"}]